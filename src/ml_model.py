import gensim
from gensim.models import KeyedVectors
import numpy as np
from typing import List, Dict, Optional, Tuple, Any, Union  # ‚úÖ –î–æ–±–∞–≤–∏–ª–∏ Union
from pathlib import Path
import json
from logger import get_logger

class MLClassifier:
    def __init__(self, models_dir: str = "models", use_pretrained: bool = True):
        self.logger = get_logger()
        # –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –±—ã—Ç—å Word2Vec –∏–ª–∏ KeyedVectors
        self.model: Optional[Union[gensim.models.Word2Vec, KeyedVectors]] = None
        self.is_trained = False
        self.category_vectors: Dict[str, np.ndarray] = {}
        self.models_dir = Path(models_dir)
        self.use_pretrained = use_pretrained
        self.is_pretrained = False  # –§–ª–∞–≥ –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        
    def train_word2vec(self, training_data: Dict[str, List[str]]) -> bool:
        """–û–±—É—á–∞–µ—Ç Word2Vec –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            all_sentences = []
            for category_texts in training_data.values():
                for text in category_texts:
                    tokens = self._tokenize_text(text)
                    all_sentences.append(tokens)
            
            if not all_sentences:
                self.logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è Word2Vec")
                return False
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = gensim.models.Word2Vec(
                sentences=all_sentences,
                vector_size=100,
                window=5,
                min_count=1,
                workers=4,
                sg=1  # skip-gram
            )
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ –î–û —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            # —á—Ç–æ–±—ã text_to_vector –º–æ–≥ —Ä–∞–±–æ—Ç–∞—Ç—å
            self.is_trained = True
            
            # –°–æ–∑–¥–∞–µ–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            self._create_category_vectors(training_data)
            self.logger.info("‚úÖ Word2Vec –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è Word2Vec: {e}")
            return False
    
    def _create_category_vectors(self, training_data: Dict[str, List[str]]):
        """–°–æ–∑–¥–∞–µ—Ç —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        self.logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π...")
        
        for category, texts in training_data.items():
            self.logger.info(f"   –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é '{category}': {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            category_vectors = []
            
            for i, text in enumerate(texts):
                text_vector = self.text_to_vector(text)
                if text_vector is not None:
                    category_vectors.append(text_vector)
                    self.logger.info(f"     –¢–µ–∫—Å—Ç {i+1}: –≤–µ–∫—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω (–¥–ª–∏–Ω–∞: {len(text_vector)})")
                else:
                    self.logger.warning(f"     –¢–µ–∫—Å—Ç {i+1}: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä")
            
            if category_vectors:
                self.category_vectors[category] = np.mean(category_vectors, axis=0)
                self.logger.info(f"   ‚úÖ –í–µ–∫—Ç–æ—Ä –¥–ª—è '{category}' —Å–æ–∑–¥–∞–Ω (–¥–ª–∏–Ω–∞: {len(self.category_vectors[category])})")
            else:
                self.logger.error(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä –¥–ª—è '{category}' - –Ω–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")
    
    def _get_word_vectors(self, word: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞, —É—á–∏—Ç—ã–≤–∞—è —Ç–∏–ø –º–æ–¥–µ–ª–∏ (Word2Vec –∏–ª–∏ KeyedVectors)"""
        if self.is_pretrained:
            # –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (KeyedVectors)
            if word in self.model.key_to_index:
                return self.model[word]
        else:
            # –û–±—ã—á–Ω–∞—è –º–æ–¥–µ–ª—å (Word2Vec)
            if hasattr(self.model, 'wv') and word in self.model.wv.key_to_index:
                return self.model.wv[word]
        return None
    
    def _find_word_in_vocab(self, word: str) -> Optional[str]:
        """–ò—â–µ—Ç —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç"""
        # –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫
        if self.is_pretrained:
            if word in self.model.key_to_index:
                return word
        else:
            if hasattr(self.model, 'wv') and word in self.model.wv.key_to_index:
                return word
        
        # –ï—Å–ª–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º–∏ —Ç–µ–≥–∞–º–∏
        if self.is_pretrained:
            # –ü—Ä–æ–±—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ç–µ–≥–∞–º–∏
            tags = ['_NOUN', '_VERB', '_ADJ', '_ADV', '_PRON', '_DET', '_PREP', '_CONJ']
            for tag in tags:
                word_with_tag = word + tag
                if word_with_tag in self.model.key_to_index:
                    return word_with_tag
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –±–∞–∑–æ–≤—ã–µ —Ñ–æ—Ä–º—ã (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è)
        variants = [
            word,  # –æ—Ä–∏–≥–∏–Ω–∞–ª
            word.rstrip('—É–µ—ã–∞–æ—ç—è–∏—é'),  # –±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≥–ª–∞—Å–Ω–æ–π
            word.rstrip('—É–µ—ã–∞–æ—ç—è–∏—é').rstrip('—É–µ—ã–∞–æ—ç—è–∏—é'),  # –±–µ–∑ –¥–≤—É—Ö –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –≥–ª–∞—Å–Ω—ã—Ö
        ]
        
        for variant in variants:
            if not variant:
                continue
                
            if self.is_pretrained:
                if variant in self.model.key_to_index:
                    return variant
                # –ü—Ä–æ–±—É–µ–º —Å —Ç–µ–≥–∞–º–∏
                for tag in tags:
                    variant_with_tag = variant + tag
                    if variant_with_tag in self.model.key_to_index:
                        return variant_with_tag
            else:
                if hasattr(self.model, 'wv') and variant in self.model.wv.key_to_index:
                    return variant
        
        return None
    
    def text_to_vector(self, text: str) -> Optional[np.ndarray]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –≤–µ–∫—Ç–æ—Ä"""
        if not self.is_trained or self.model is None or not text:
            self.logger.debug(f"text_to_vector: –º–æ–¥–µ–ª—å –Ω–µ –≥–æ—Ç–æ–≤–∞ –∏–ª–∏ —Ç–µ–∫—Å—Ç –ø—É—Å—Ç–æ–π (is_trained={self.is_trained}, model={self.model is not None}, text={bool(text)})")
            return None
        
        tokens = self._tokenize_text(text)
        self.logger.debug(f"–¢–æ–∫–µ–Ω—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {tokens}")
        
        if not tokens:
            self.logger.debug("–ù–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
            return None
        
        vectors = []
        
        for token in tokens:
            # –ò—â–µ–º —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–∞—Ä–µ (—Å —É—á–µ—Ç–æ–º –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ç–µ–≥–æ–≤)
            found_word = self._find_word_in_vocab(token)
            if found_word:
                vector = self._get_word_vectors(found_word)
                if vector is not None:
                    vectors.append(vector)
                    self.logger.debug(f"–¢–æ–∫–µ–Ω '{token}' –Ω–∞–π–¥–µ–Ω –∫–∞–∫ '{found_word}'")
            else:
                self.logger.debug(f"–¢–æ–∫–µ–Ω '{token}' –ù–ï –Ω–∞–π–¥–µ–Ω –≤ —Å–ª–æ–≤–∞—Ä–µ")
        
        self.logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {len(vectors)} –∏–∑ {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        if vectors:
            result = np.mean(vectors, axis=0)
            self.logger.debug(f"–í–µ–∫—Ç–æ—Ä —Å–æ–∑–¥–∞–Ω (–¥–ª–∏–Ω–∞: {len(result)})")
            return result
        else:
            self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä - –Ω–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ. –¢–æ–∫–µ–Ω—ã: {tokens}")
            return None
    
    def predict_category(self, text: str) -> Tuple[Optional[str], float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é"""
        if not self.is_trained or not self.category_vectors:
            return None, 0.0
        
        text_vector = self.text_to_vector(text)
        if text_vector is None:
            return None, 0.0
        
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏
        best_category = None
        best_similarity = -1.0
        
        for category, category_vector in self.category_vectors.items():
            similarity = self._cosine_similarity(text_vector, category_vector)
            if similarity > best_similarity:
                best_similarity = similarity
                best_category = category
        
        return best_category, best_similarity
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _tokenize_text(self, text: str) -> List[str]:
        """–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        import re
        
        if not text:
            return []
        
        # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ - —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        text_clean = re.sub(r'[^\w\s]', ' ', text.lower())
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        tokens = text_clean.split()
        
        # –ù–ï —É–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ - –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∞–∂–Ω—ã!
        # tokens = [token for token in tokens if len(token) > 2]
        
        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ —É–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–≤—Å–µ–º –ø—É—Å—Ç—ã–µ —Ç–æ–∫–µ–Ω—ã
        tokens = [token for token in tokens if token.strip()]
        
        self.logger.debug(f"–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è: '{text[:50]}...' -> {len(tokens)} —Ç–æ–∫–µ–Ω–æ–≤")
        
        return tokens
    
    def save_model(self) -> bool:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –≤ –ø–∞–ø–∫—É models/"""
        try:
            if self.model is None:
                self.logger.error("‚ùå –ù–µ—á–µ–≥–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å: –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
                return False
            
            # –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (KeyedVectors) –Ω–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å - –æ–Ω–∏ —É–∂–µ –µ—Å—Ç—å
            if self.is_pretrained:
                self.logger.info("üí° –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —É–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞, —Å–æ—Ö—Ä–∞–Ω—è—é —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
            else:
                self.models_dir.mkdir(parents=True, exist_ok=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º Word2Vec –º–æ–¥–µ–ª—å
                model_path = self.models_dir / "word2vec.model"
                self.model.save(str(model_path))
                self.logger.info(f"üíæ Word2Vec –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {model_path}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–í–ê–ñ–ù–û: –¥–∞–∂–µ –µ—Å–ª–∏ –ø—É—Å—Ç—ã–µ)
            self.models_dir.mkdir(parents=True, exist_ok=True)
            vectors_data = {
                category: vector.tolist() 
                for category, vector in self.category_vectors.items()
            }
            
            vectors_path = self.models_dir / "category_vectors.json"
            with open(vectors_path, 'w', encoding='utf-8') as f:
                json.dump(vectors_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üíæ –í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {list(vectors_data.keys())}")
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def load_pretrained_model(self, model_name: str = "word2vec-ruscorpora-300.model") -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (KeyedVectors)"""
        try:
            model_path = self.models_dir / model_name
            
            if not model_path.exists():
                self.logger.warning(f"–ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return False
            
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞–∫ KeyedVectors (–ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)
            try:
                self.model = KeyedVectors.load(str(model_path))
                self.is_pretrained = True
                self.logger.info(f"üì• –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
                self.logger.info(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.model.key_to_index)} —Å–ª–æ–≤")
                self.logger.info(f"   –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞: {self.model.vector_size}")
            except:
                # –ï—Å–ª–∏ –Ω–µ KeyedVectors, –ø—Ä–æ–±—É–µ–º –∫–∞–∫ Word2Vec
                self.model = gensim.models.Word2Vec.load(str(model_path))
                self.is_pretrained = False
                self.logger.info(f"üì• Word2Vec –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
            
            self.is_trained = True
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def load_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–ø–∫–∏ models/ (—Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é, –ø–æ—Ç–æ–º —Å–≤–æ—é)"""
        try:
            vectors_path = self.models_dir / "category_vectors.json"
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            if self.use_pretrained:
                pretrained_path = self.models_dir / "word2vec-ruscorpora-300.model"
                if pretrained_path.exists():
                    if self.load_pretrained_model("word2vec-ruscorpora-300.model"):
                        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
                        if vectors_path.exists():
                            with open(vectors_path, 'r', encoding='utf-8') as f:
                                vectors_data = json.load(f)
                            
                            self.category_vectors = {
                                category: np.array(vector) 
                                for category, vector in vectors_data.items()
                            }
                            if self.category_vectors:
                                self.logger.info(f"üì• –í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {list(vectors_data.keys())}")
                            else:
                                self.logger.warning("–í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –ø—É—Å—Ç—ã–µ - –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å")
                        else:
                            self.logger.warning("–í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å")
                        return True
            
            # –ï—Å–ª–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º —Å–≤–æ—é –º–æ–¥–µ–ª—å
            model_path = self.models_dir / "word2vec.model"
            
            if not model_path.exists():
                self.logger.warning("Word2Vec –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º Word2Vec –º–æ–¥–µ–ª—å
            self.model = gensim.models.Word2Vec.load(str(model_path))
            self.is_pretrained = False
            self.logger.info(f"üì• Word2Vec –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if vectors_path.exists():
                with open(vectors_path, 'r', encoding='utf-8') as f:
                    vectors_data = json.load(f)
                
                self.category_vectors = {
                    category: np.array(vector) 
                    for category, vector in vectors_data.items()
                }
                self.logger.info(f"üì• –í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {list(vectors_data.keys())}")
            else:
                self.logger.warning("–í–µ–∫—Ç–æ—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                self.category_vectors = {}
            
            self.is_trained = True
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º Any –≤–º–µ—Å—Ç–æ any
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏"""
        if self.model is None:
            return {"status": "not_trained"}
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –∏ –≤–µ–∫—Ç–æ—Ä–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
        if self.is_pretrained:
            vocab_size = len(self.model.key_to_index)
            vector_size = self.model.vector_size
            model_type = "pretrained (KeyedVectors)"
        else:
            vocab_size = len(self.model.wv.key_to_index) if hasattr(self.model, 'wv') else 0
            vector_size = self.model.vector_size if hasattr(self.model, 'vector_size') else 0
            model_type = "trained (Word2Vec)"
        
        return {
            "status": "trained" if self.is_trained else "not_trained",
            "model_type": model_type,
            "vocabulary_size": vocab_size,
            "categories": list(self.category_vectors.keys()),
            "vector_size": vector_size
        }